{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have already loaded the BACI data\n",
    "BACI_PATH = \"path/to/your/BACI_HS92_V202201.csv\"\n",
    "df = pd.read_csv(BACI_PATH)\n",
    "\n",
    "# Rename columns for clarity (as in the previous example)\n",
    "df = df.rename(columns={\n",
    "    't': 'year',\n",
    "    'i': 'exporter',\n",
    "    'j': 'importer',\n",
    "    'k': 'product_code',\n",
    "    'v': 'trade_value_thousands_usd',\n",
    "    'q': 'quantity_tons'\n",
    "})\n",
    "\n",
    "df['trade_value_millions_usd'] = df['trade_value_thousands_usd'] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0.00/9.85G [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0.00/9.85G [00:00<?, ?B/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'product_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/15/02jzgm713tnfvvysvvpt2t800000gn/T/ipykernel_21685/917235795.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Read and process the file in chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             processed_chunk = process_chunk(\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountry_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0msampled_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             sampled_data = pd.concat(\n",
      "\u001b[0;32m/var/folders/15/02jzgm713tnfvvysvvpt2t800000gn/T/ipykernel_21685/917235795.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(chunk, country_codes, product_codes)\u001b[0m\n\u001b[1;32m     26\u001b[0m     chunk = pd.merge(chunk, country_codes, left_on='j',\n\u001b[1;32m     27\u001b[0m                      \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'country_code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_j'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Merge with product codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     chunk = pd.merge(chunk, product_codes, left_on='k',\n\u001b[0m\u001b[1;32m     31\u001b[0m                      \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'product_code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/TradeThesis_Helfrich_Final/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/TradeThesis_Helfrich_Final/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/TradeThesis_Helfrich_Final/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/TradeThesis_Helfrich_Final/.venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'product_code'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "csv_directory = '/Users/ian/Downloads/TradeThesis_Helfrich_Final/data/raw/cepii/cepii_baci_hs02_v202401b/'\n",
    "output_file = 'sampled_baci_data.csv'\n",
    "\n",
    "# Get list of CSV files, excluding country and product code files\n",
    "csv_files = [f for f in glob.glob(\n",
    "    f'{csv_directory}*.csv') if 'country_codes' not in f and 'product_codes' not in f]\n",
    "\n",
    "# Load country and product code data\n",
    "country_codes = pd.read_csv(f'{csv_directory}country_codes_V202401b.csv')\n",
    "product_codes = pd.read_csv(f'{csv_directory}product_codes_HS02_V202401b.csv')\n",
    "\n",
    "# Function to process a chunk of data\n",
    "\n",
    "\n",
    "def process_chunk(chunk, country_codes, product_codes):\n",
    "    # Merge with country codes (for both i and j)\n",
    "    chunk = pd.merge(chunk, country_codes, left_on='i',\n",
    "                     right_on='country_code', how='left')\n",
    "    chunk = pd.merge(chunk, country_codes, left_on='j',\n",
    "                     right_on='country_code', how='left', suffixes=('_i', '_j'))\n",
    "\n",
    "    # Merge with product codes\n",
    "    chunk = pd.merge(chunk, product_codes, left_on='k',\n",
    "                     right_on='product_code', how='left')\n",
    "\n",
    "    return chunk\n",
    "\n",
    "# Function to sample 1% of data\n",
    "\n",
    "\n",
    "def sample_chunk(chunk, sample_frac=0.01):\n",
    "    return chunk.sample(frac=sample_frac)\n",
    "\n",
    "\n",
    "# Initialize empty DataFrame for the sample\n",
    "sampled_data = pd.DataFrame()\n",
    "\n",
    "# Process files\n",
    "total_size = sum(os.path.getsize(f) for f in csv_files)\n",
    "with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Processing files\") as pbar:\n",
    "    for file in csv_files:\n",
    "        # Extract year from filename\n",
    "        year = int(file.split('_Y')[-1].split('_')[0])\n",
    "\n",
    "        # Read and process the file in chunks\n",
    "        for chunk in pd.read_csv(file, chunksize=100000):\n",
    "            chunk['year'] = year\n",
    "            processed_chunk = process_chunk(\n",
    "                chunk, country_codes, product_codes)\n",
    "            sampled_chunk = sample_chunk(processed_chunk)\n",
    "            sampled_data = pd.concat(\n",
    "                [sampled_data, sampled_chunk], ignore_index=True)\n",
    "\n",
    "            pbar.update(chunk.memory_usage(deep=True).sum())\n",
    "\n",
    "# Save the sampled data\n",
    "sampled_data.to_csv(output_file, index=False)\n",
    "print(f\"Sampled data (1%) saved to '{output_file}'\")\n",
    "\n",
    "# Display summary of the sampled data\n",
    "print(\"\\nSampled Data Summary:\")\n",
    "print(sampled_data.info())\n",
    "print(\"\\nSample of the data:\")\n",
    "print(sampled_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 22.7GB [08:50, 42.7MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data (1%) saved to 'sampled_baci_data.csv'\n",
      "\n",
      "Sampled Data Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2060489 entries, 0 to 2060488\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   t       int64  \n",
      " 1   i       int64  \n",
      " 2   j       int64  \n",
      " 3   k       int64  \n",
      " 4   v       float64\n",
      " 5   q       object \n",
      " 6   year    int64  \n",
      "dtypes: float64(1), int64(5), object(1)\n",
      "memory usage: 110.0+ MB\n",
      "None\n",
      "\n",
      "Sample of the data:\n",
      "      t   i    j       k       v              q  year\n",
      "0  2021   8  380  281511  11.488          6.935  2021\n",
      "1  2021  32  784  848590  12.684          0.058  2021\n",
      "2  2021  32  214  870870   6.900             NA  2021\n",
      "3  2021  32   56  871690  11.921          1.750  2021\n",
      "4  2021   8  616  611120   0.182          0.012  2021\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def analyze_csv(file_path):\n",
    "    \"\"\"Analyze a CSV file and return its structure.\"\"\"\n",
    "    df = pd.read_csv(\n",
    "        file_path, nrows=1000)  # Read first 1000 rows for analysis\n",
    "    return {\n",
    "        'columns': list(df.columns),\n",
    "        'dtypes': df.dtypes.to_dict(),\n",
    "        'sample': df.head(5).to_dict()\n",
    "    }\n",
    "\n",
    "\n",
    "def find_merge_keys(main_structure, auxiliary_structure):\n",
    "    \"\"\"Find potential merge keys between main and auxiliary dataframes.\"\"\"\n",
    "    main_cols = set(main_structure['columns'])\n",
    "    aux_cols = set(auxiliary_structure['columns'])\n",
    "\n",
    "    # Find common columns\n",
    "    common_cols = main_cols.intersection(aux_cols)\n",
    "\n",
    "    # Check for columns that might be renamed (e.g., 'i' in main, 'country_code' in auxiliary)\n",
    "    potential_renames = []\n",
    "    for main_col in main_cols:\n",
    "        for aux_col in aux_cols:\n",
    "            if main_structure['dtypes'][main_col] == auxiliary_structure['dtypes'][aux_col]:\n",
    "                if main_structure['sample'][main_col].values() == auxiliary_structure['sample'][aux_col].values():\n",
    "                    potential_renames.append((main_col, aux_col))\n",
    "\n",
    "    return list(common_cols), potential_renames\n",
    "\n",
    "\n",
    "def process_chunk(chunk, auxiliary_dfs, merge_info):\n",
    "    \"\"\"Process a chunk of the main dataframe, merging with auxiliary data.\"\"\"\n",
    "    for aux_name, merge_keys in merge_info.items():\n",
    "        left_on, right_on = merge_keys\n",
    "        chunk = pd.merge(\n",
    "            chunk, auxiliary_dfs[aux_name], left_on=left_on, right_on=right_on, how='left')\n",
    "    return chunk\n",
    "\n",
    "\n",
    "# Define paths\n",
    "csv_directory = '/Users/ian/Downloads/TradeThesis_Helfrich_Final/data/raw/cepii/cepii_baci_hs02_v202401b/'\n",
    "output_file = 'sampled_baci_data.csv'\n",
    "\n",
    "# Get list of CSV files\n",
    "csv_files = glob.glob(f'{csv_directory}*.csv')\n",
    "\n",
    "# Analyze all CSV files\n",
    "file_structures = {file: analyze_csv(file) for file in csv_files}\n",
    "\n",
    "# Identify main data files and auxiliary files\n",
    "main_files = [f for f in csv_files if 'BACI_HS02_Y' in f]\n",
    "auxiliary_files = [f for f in csv_files if f not in main_files]\n",
    "\n",
    "# Analyze main data structure (using the first main file)\n",
    "main_structure = file_structures[main_files[0]]\n",
    "\n",
    "# Find merge keys for auxiliary files\n",
    "merge_info = {}\n",
    "auxiliary_dfs = {}\n",
    "for aux_file in auxiliary_files:\n",
    "    aux_structure = file_structures[aux_file]\n",
    "    common_cols, potential_renames = find_merge_keys(\n",
    "        main_structure, aux_structure)\n",
    "\n",
    "    if common_cols:\n",
    "        merge_info[aux_file] = (common_cols[0], common_cols[0])\n",
    "    elif potential_renames:\n",
    "        merge_info[aux_file] = potential_renames[0]\n",
    "\n",
    "    auxiliary_dfs[aux_file] = pd.read_csv(aux_file)\n",
    "\n",
    "# Initialize empty DataFrame for the sample\n",
    "sampled_data = pd.DataFrame()\n",
    "\n",
    "# Process files\n",
    "total_size = sum(os.path.getsize(f) for f in main_files)\n",
    "with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Processing files\") as pbar:\n",
    "    for file in main_files:\n",
    "        # Extract year from filename\n",
    "        year = int(file.split('_Y')[-1].split('_')[0])\n",
    "\n",
    "        # Read and process the file in chunks\n",
    "        for chunk in pd.read_csv(file, chunksize=100000):\n",
    "            chunk['year'] = year\n",
    "            processed_chunk = process_chunk(chunk, auxiliary_dfs, merge_info)\n",
    "            sampled_chunk = processed_chunk.sample(frac=0.01)  # 1% sample\n",
    "            sampled_data = pd.concat(\n",
    "                [sampled_data, sampled_chunk], ignore_index=True)\n",
    "\n",
    "            pbar.update(chunk.memory_usage(deep=True).sum())\n",
    "\n",
    "# Save the sampled data\n",
    "sampled_data.to_csv(output_file, index=False)\n",
    "print(f\"Sampled data (1%) saved to '{output_file}'\")\n",
    "\n",
    "# Display summary of the sampled data\n",
    "print(\"\\nSampled Data Summary:\")\n",
    "print(sampled_data.info())\n",
    "print(\"\\nSample of the data:\")\n",
    "print(sampled_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "      t  i   j       k       v       q  country_code exporter_name  \\\n",
      "0  2021  4  20  420229   0.598   0.001             4   Afghanistan   \n",
      "1  2021  4  20  621050   0.200   0.001             4   Afghanistan   \n",
      "2  2021  4  20  640319   0.167   0.001             4   Afghanistan   \n",
      "3  2021  4  20  870899   0.461   0.001             4   Afghanistan   \n",
      "4  2021  4  24  382490  53.015  13.235             4   Afghanistan   \n",
      "\n",
      "  exporter_iso2 exporter_iso3  country_code_importer importer_name  \\\n",
      "0            AF           AFG                     20       Andorra   \n",
      "1            AF           AFG                     20       Andorra   \n",
      "2            AF           AFG                     20       Andorra   \n",
      "3            AF           AFG                     20       Andorra   \n",
      "4            AF           AFG                     24        Angola   \n",
      "\n",
      "  importer_iso2 importer_iso3    code  \\\n",
      "0            AD           AND  420229   \n",
      "1            AD           AND  621050   \n",
      "2            AD           AND  640319   \n",
      "3            AD           AND  870899   \n",
      "4            AO           AGO  382490   \n",
      "\n",
      "                                         description  \n",
      "0  Cases and containers: handbags (whether or not...  \n",
      "1  Garments: women's or girls', n.e.s. in item no...  \n",
      "2  Sports footwear: (other than ski-boots, snowbo...  \n",
      "3  Vehicles: parts and accessories, n.e.s. in hea...  \n",
      "4  Chemical products, preparations and residual p...  \n",
      "\n",
      "Data types:\n",
      "t                          int64\n",
      "i                          int64\n",
      "j                          int64\n",
      "k                         object\n",
      "v                        float64\n",
      "q                        float64\n",
      "country_code               int64\n",
      "exporter_name             object\n",
      "exporter_iso2             object\n",
      "exporter_iso3             object\n",
      "country_code_importer      int64\n",
      "importer_name             object\n",
      "importer_iso2             object\n",
      "importer_iso3             object\n",
      "code                      object\n",
      "description               object\n",
      "dtype: object\n",
      "\n",
      "Shape of the DataFrame:\n",
      "(206048851, 16)\n",
      "\n",
      "Descriptive statistics:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your parquet file\n",
    "file_path = '/Users/ian/Downloads/TradeThesis_Helfrich_Final/data/raw/cepii/cepii_gravity_loadingCodeExperiment/test11.parquet'\n",
    "\n",
    "# Read the parquet file\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nShape of the DataFrame:\")\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
